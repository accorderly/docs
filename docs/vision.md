# Rationale

An absence of knowledge engineering to manage data science causes models to become biased and inflexible. This is known as [concept drift](https://en.wikipedia.org/wiki/Concept_drift). Products are theories of reality, and science already knows how to evolve theories. When support for new business domains need to be added to a product that includes learning, there needs to be an organized label structure to query the annotated training data through, otherwise models will calcify and become unable to adapt. Just like regular code debt caused by tightly coupled concerns.

There are many kinds of models that fit different problems and not everything needs to be encoded into any one particular type of classifier. Feature engineering is important of course, but shoehorning every blob of dimensionality into a single uniform structure for all types of predictions is asking for trouble. As with business logic, classification modules should be orthogonal, pluggable adapters that implement interfaces. We ask chefs about cooking and bakers about baking. They are both specialized at their respective (sub)domains. Even though they share common ingredients and tools, they operate under different rules using different jargon.

To manage the challenges of model bias and over fitting that stem from unstructured target labels and neglect of semantics, one general solution is to train multiple models. That is, one model for each domain or class of facts that is possible to maintain a high level of accuracy about. Then reasoning can be externalized by deciding which model has more authority in a given context, making classifiers more programmable.

Reyearn uses a forest of taxonomic trees, a directed graph. Each branch in a tree represents a nested set of labelled observations. These observations are leaves on a tree (at the very edge of the graph). This label tree idea is related to [Hierarchical Multilabel Classification](https://en.wikipedia.org/wiki/Multiclass_classification#Hierarchical_classification). It's programmed at the application level rather than being used to structure word embeddings inside statistical models (although that is an interesting technique on its own). The ensemble of models approach has been demonstrated by [MILABOT](https://arxiv.org/abs/1709.02349) for the Alexa Prize, [Microsoft LUIS](https://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-concept-prebuilt-model#prebuilt-domains) and others.

Making the ensemble of models configurable by data at runtime allows users to shape the behavior of the system, and the operational overhead of training and deploying very large models is reduced. It also allows us to mix vastly different types of models more easily, e.g. some can be CNNs, RNNs, some Bayesian, some expression parsers, rule matchers etc. Any function or third party endpoint can act as a classifier in principle.

## Other projects

There is so much activity in the area of open source ML that it is hard to keep up. This is a running list of similar projects doing things in different but complimentary ways.

- [Creme](https://github.com/creme-ml/creme): Solving the concept drift problem at a lower level. A library focussed on streaming incremental/continuous learning, with great ML algorithm support.
- [Weights and Biases](https://docs.wandb.com/): A commercial product with full UI for experiment management.
- [ConceptNet Numberbatch](https://github.com/commonsense/conceptnet-numberbatch): training general purpose language models by querying for training data through a semantic network. The semantic qualities are great for combatting model bias because samples can be balanced by conceptual meanings.
- [Grakn KGLIB](https://github.com/graknlabs/kglib): training models by querying for training data through arbitrary knowledge (property) graphs with deductive logic.